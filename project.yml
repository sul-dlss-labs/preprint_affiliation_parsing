title: Affiliation Extraction and Parsing from OpenALEX Preprints

vars:
  embedding: "transformer" # tok2vec, transformer
  transformer_model_name: "roberta-base" # any huggingface model name
  gpu_id: 0
  config_file: "configs/config_${embedding}.cfg"

directories:
  - assets
  - training
  - results

assets:
  - dest: "assets/preprints.csv"
    git:
      repo: https://github.com/sul-dlss-labs/preprints-evaluation-dataset.git
      branch: main
      path: records.csv
    checksum: 68138588160e22998f6e6921f4e705d8 # md5
    description: List of preprint documents in the training dataset

workflows:
  prepare:
    - preprints:download
    - preprints:clean
    - dataset:extraction:create

commands:
  - name: dependencies:install
    help: Install dependencies
    script:
      - pip install -r requirements.txt
      - python -m spacy download en_core_web_lg # for suggestion & segmentation
      - /bin/bash -c "if [[ $(arch) == 'arm64' ]]; then pip install thinc-apple-ops; fi" # for M-series macs

  - name: preprints:download
    help: Download preprint PDFs and metadata (**Needs Stanford VPN**)
    outputs:
      - assets/preprints/pdf
    script:
      - python scripts/download_preprints.py assets/preprints.csv assets/preprints/pdf
      - python scripts/download_metadata.py assets/preprints.csv assets/preprints/json

  - name: preprints:clean
    help: Extract plain text from preprint PDFs
    deps:
      - assets/preprints/pdf
    outputs:
      - assets/preprints/txt
    script:
      - python scripts/clean_preprints.py assets/preprints/pdf assets/preprints/txt

  - name: dataset:extraction:create
    help: Create a dataset for extracting affiliations
    deps:
      - assets/preprints/txt
    outputs:
      - assets/preprints_textcat.jsonl
    script:
      - python scripts/create_extraction_dataset.py assets/preprints_textcat.jsonl

  - name: annotate:extraction
    help: Annotate binary training data for extraction
    deps:
      - assets/preprints_textcat.jsonl
    script:
      - python -m prodigy textcat.manual preprints_textcat assets/preprints_textcat.jsonl --label AFFILIATION

  - name: dataset:parsing:create
    help: Create a dataset for parsing affiliations
    deps:
      - assets/preprints/txt
    outputs:
      - assets/preprints.jsonl
    script:
      - python scripts/create_parsing_dataset.py assets/preprints.jsonl

  - name: annotate:ner
    help: Annotate training data for NER by correcting and updating an existing model
    deps:
      - assets/preprints.jsonl
    script:
      - python -m prodigy ner.correct preprints_ner en_core_web_lg assets/preprints.jsonl --label PERSON,ORG,GPE --update

  - name: annotate:spans
    help: Manually annotate training data for spans and relations
    deps:
      - assets/preprints.jsonl
    script:
      - python -m prodigy rel.manual preprints_spans en_core_web_lg assets/preprints.jsonl --label HAS --span-label AUTHOR,AFFILIATION --wrap

  - name: train_extract
    help: Train spaCy pipeline for affiliation extraction
    deps:
      - "configs/extract/${vars.config_file}"
      - corpus/train.spacy
      - corpus/dev.spacy
    script:
      - "python -m spacy train configs/extract/${vars.config_file} --output training/parse --gpu-id ${vars.gpu_id} --vars.transformer_model_name ${vars.transformer_model_name}"

  - name: train_parse
    help: Train spaCy pipeline for affiliation parsing
    deps:
      - "configs/parse/${vars.config_file}"
      - corpus/train.spacy
      - corpus/dev.spacy
    script:
      - "python -m spacy train configs/parse/${vars.config_file} --output training --gpu-id ${vars.gpu_id} --vars.transformer_model_name ${vars.transformer_model_name}"

  - name: apply
    help: Apply the best trained model to the data and save the results
    deps:
      - training/model-best
      - assets/preprints/txt
    outputs:
      - results/preprints.spacy
    script:
      - "python -m spacy apply training/model-best assets/preprints/txt results/preprints.spacy --gpu-id ${vars.gpu_id}"

  - name: visualize
    help: Visualize and compare results from different models on the data
    deps:
      - training/model-best
      - assets/preprints.jsonl
    script:
      - "python -m streamlit run scripts/visualize.py"
