{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipeline evaluation\n",
    "\n",
    "This notebook is used to evaluate the entire pipeline. It compares the predictions of the pipeline with the ground truth author and affiliation data, along with predictions made using other strategies.\n",
    "\n",
    "Ground truth authors and affiliations were cataloged by hand using SHROOM, and are downloaded as Cocina from SDR by the `preprints:download` task (see README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/budak/.pyenv/versions/3.12.2/envs/ezdeposit/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/budak/.pyenv/versions/3.12.2/envs/ezdeposit/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'analyze_pdf' from 'api' (/Users/budak/Developer/preprint_affiliation_parsing/scripts/api.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# make scripts in scripts/ importable and import the analysis pipeline\u001b[39;00m\n\u001b[1;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mstr\u001b[39m(PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscripts\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pdf\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cocina_affiliations\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the models\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'analyze_pdf' from 'api' (/Users/budak/Developer/preprint_affiliation_parsing/scripts/api.py)"
     ]
    }
   ],
   "source": [
    "# set up project root path for imports\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "PROJECT_ROOT = pathlib.Path(root)\n",
    "\n",
    "# make scripts in scripts/ importable and import the analysis pipeline\n",
    "sys.path.insert(1, str(PROJECT_ROOT / 'scripts'))\n",
    "from utils import get_cocina_affiliations\n",
    "\n",
    "# Load the models\n",
    "import spacy\n",
    "ner = spacy.load(\"en_core_web_trf\")\n",
    "ner.disable_pipes(\"parser\")\n",
    "textcat = spacy.load(PROJECT_ROOT / 'training' / 'textcat' / 'model-best')\n",
    "\n",
    "# convenience function for fetching preprint text\n",
    "def get_preprint_text(preprint_id):\n",
    "    fp = PROJECT_ROOT / \"assets\" / \"preprints\" / \"txt\" / f\"{preprint_id}.txt\"\n",
    "    try:\n",
    "        return fp.read_text(encoding='utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Preprint text not found for {preprint_id}\")\n",
    "        return \"\"\n",
    "\n",
    "# convenience function for fetching gold affiliations from cocina\n",
    "import json\n",
    "def get_gold_affiliations(preprint_id):\n",
    "    fp = PROJECT_ROOT / \"assets\" / \"preprints\" / \"json\" / f\"{preprint_id}.json\"\n",
    "    try:\n",
    "        json_str = fp.read_text(encoding='utf-8')\n",
    "        cocina = json.loads(json_str)\n",
    "        return get_cocina_affiliations(cocina)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Cocina data not found for {preprint_id}\")\n",
    "        return \"\"\n",
    "    \n",
    "# convenience function for loading pre-saved predictions from results/\n",
    "results_path = PROJECT_ROOT / 'results'\n",
    "def load_predictions():\n",
    "    prediction_files = list(results_path.glob(\"*.json\"))\n",
    "    predictions = {}\n",
    "    for prediction_file in prediction_files:\n",
    "        preprint_id = prediction_file.stem\n",
    "        with prediction_file.open(mode=\"r\") as f:\n",
    "            try:\n",
    "                contents = json.load(f)\n",
    "                predictions[preprint_id] = contents\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error loading {prediction_file}\")\n",
    "                continue\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data table with columns for gold and predicted affiliations\n",
    "import pandas as pd\n",
    "preprints = pd.read_csv(PROJECT_ROOT / 'assets' / 'preprints.csv')\n",
    "preprints['gold'] = ''\n",
    "\n",
    "# add the full text and gold affiliations to the data table\n",
    "for i, row in preprints.iterrows():\n",
    "    openalex_url = row['OpenAlex ID']\n",
    "    preprint_id = openalex_url.split('/')[-1]\n",
    "    preprint_text = get_preprint_text(preprint_id)\n",
    "    preprint_file = PROJECT_ROOT / \"assets\" / \"preprints\" / \"pdf\" / f\"{preprint_id}.pdf\"\n",
    "    preprints.at[i, 'gold'] = get_gold_affiliations(preprint_id)\n",
    "    preprints.at[i, 'text'] = preprint_text\n",
    "    \n",
    "\n",
    "# keep only the columns we need\n",
    "preprints = preprints[['OpenAlex ID', 'DRUID', 'text', 'gold']]\n",
    "\n",
    "# limit to only rows where we have gold affiliations\n",
    "preprints = preprints[preprints['gold'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_affiliation_dict, analyze_pdf_text\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# set this and run cell to force re-running predictions\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import get_affiliation_dict, analyze_pdf_text\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# set this and run cell to force re-running predictions\n",
    "FORCE_RERUN = False\n",
    "\n",
    "# add a column for predictions\n",
    "preprints['pred'] = ''\n",
    "\n",
    "# if we don't have any saved predictions, run prediction for every preprint\n",
    "predictions = load_predictions()\n",
    "if not predictions or FORCE_RERUN:\n",
    "    print(\"No predictions found, running prediction for all preprints\")\n",
    "    for i, row in tqdm(preprints.iterrows(), total=len(preprints), desc=\"Predicting\"):\n",
    "        preprint_id = row['OpenAlex ID'].split('/')[-1]\n",
    "        preprint_file = PROJECT_ROOT / \"assets\" / \"preprints\" / \"txt\" / f\"{preprint_id}.txt\"\n",
    "        pdf_text = preprint_file.read_text(encoding='utf-8')\n",
    "        try:\n",
    "            result = analyze_pdf_text(pdf_text, textcat, ner)\n",
    "            affiliations = get_affiliation_dict(result)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error analyzing {preprint_id}: {e}\")\n",
    "            affiliations = {}\n",
    "        with (results_path / f\"{preprint_id}.json\").open(mode=\"w\") as f:\n",
    "            json.dump(affiliations, f)\n",
    "    predictions = load_predictions()\n",
    "else:\n",
    "    print(\"Using saved predictions\")\n",
    "\n",
    "# set predictions for each preprint in the data table\n",
    "for i, row in preprints.iterrows():\n",
    "    preprint_id = row['OpenAlex ID'].split('/')[-1]\n",
    "    if preprint_id in predictions:\n",
    "        preprints.at[i, 'pred'] = predictions[preprint_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ezdeposit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
